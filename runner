#!/usr/bin/env python
#

from os                                 import getenv, path, makedirs, environ
from sys                                import exit
from shutil                             import rmtree, copytree
from logging                            import debug, error, basicConfig, DEBUG, WARNING
from lib.argparse                       import ArgumentParser, RawDescriptionHelpFormatter
import json
import platform

from lib.testsprops                     import TestProfiles, TestCollections
from lib.test_attributes                import TestAttributes
from lib.shell                          import sh, ShellError, ShellTimeoutError
from lib.configuration                  import Configuration

# ErrorExit
#
class ErrorExit(Exception):
    """
    If an error message has already been displayed and we want to just exit the app, this
    exception is raised.
    """
    pass

# Runner
#
class Runner():
    """
    """

    # __init__
    #
    def __init__(self, args):
        self.args = args
        self.__release_name = None
        self.scratch_needed = False
        self.test_list = None
        self.autotest_client_root = 'autotest/client'
        self.autotest_exe_path = path.join(self.autotest_client_root, 'autotest-local')
        self.autotest_tests_root = path.join(self.autotest_client_root, 'tests')
        self.saved_results_root = 'kernel-test-results'
        self.autotest_results_root = path.join(self.autotest_client_root, 'results', 'default')
        self.per_test_logs_root = ''
        self.per_test_results_root = ''
        self.arch = platform.processor()

    # tests_list_expand
    #
    def tests_list_expand(self):
        # The list of tests or the group(s) of tests that we are to
        # run is passed in on the command line.
        #
        self.tests_list = self.args.tests
        fully_expanded = False
        while not fully_expanded:
            fully_expanded = True
            neo = []
            for t in self.tests_list:
                if t in TestCollections:
                    for c in TestCollections[t]:
                        if c not in neo:
                            neo.append(c)
                        fully_expanded = False
                else:
                    if t not in neo:
                        neo.append(t)
            self.tests_list = neo

        if len(self.tests_list) > 0:
            debug('Expanded test list:')
            for t in self.tests_list:
                debug('    %s' % t)

    def blacklisted(self, test, value):
        retval = False
        bl = path.join(self.autotest_tests_root, test, 'blacklist.%s' % value)
        if path.exists(bl):
            retval = True
        return retval

    # tests_list_verify
    #
    def tests_list_verify(self):
        debug('Enter tests_list_verify')

        errors = False

        # Verify that all the tests in the list are ones that
        # we know about.
        #
        for t in self.tests_list:
            if t not in TestProfiles:
                error('Test \'%s\' is not recognized.' % t)
                errors = True

        # Verify that certain tests have not been blaclisted for the series
        # that we are currently running.
        #
        valid_tests = []
        debug('    blacklist checking (%s) (%s)' % (self.release_name, self.arch))
        for t in self.tests_list:
            debug('        \'%s\'' % t)
            if self.blacklisted(t, self.release_name):
                error('Test \'%s\' has been blacklisted for this series (%s)' % (t, self.release_name))
            elif self.blacklisted(t, self.arch):
                error('Test \'%s\' has been blacklisted for this arch (%s)' % (t, self.arch))
            else:
                debug('            valid')
                valid_tests.append(t)

        self.tests_list = valid_tests

        # Verify that the tests are valid, autotest tests.
        #
        if not path.exists(self.autotest_tests_root):
            error('The autotest client tests were not found (%s).' % (self.autotest_tests_root))
        else:
            for t in self.tests_list:
                if not path.exists(path.join(self.autotest_tests_root, t)):
                    error('The autotest test (%s) is not available in the autotest tree (%s).' % (t, self.autotest_tests_root))
                    errors = True

        if errors:
            self.tests_list = None
            raise ErrorExit()

        if len(self.tests_list) > 0:
            debug('Verified test list:')
            for t in self.tests_list:
                debug('    %s' % t)

        debug('Leave tests_list_verify')

    # release_name
    #
    @property
    def release_name(self):
        if self.__release_name is None:
            status, std_out = sh("lsb_release -a")
            for line in std_out:
                parts = line.split(":")
                if parts[0] == "Codename":
                    self.__release_name = parts[1].strip()
                    debug("Release name is %s" % self.__release_name)
                    break
        return self.__release_name

    # test_list_determine_control_files
    #
    def test_list_determine_control_files(self):
        for t in self.tests_list:
            debug("Determining correct control file for test: %s" % t)

            # Is there control file named 'control.ubuntu.<series>'?
            #
            fid = path.join(self.autotest_tests_root, t, 'control.ubuntu.%s' % self.release_name)
            if path.exists(fid):
                TestProfiles[t]['control file'] = fid
                debug("    using: %s" % fid)
                continue

            # Is there a control file named 'control.ubuntu'?
            #
            fid = path.join(self.autotest_tests_root, t, 'control.ubuntu')
            if path.exists(fid):
                TestProfiles[t]['control file'] = fid
                debug("    using: %s" % fid)
                continue

            # Is there a control file named 'control'?
            #
            fid = path.join(self.autotest_tests_root, t, 'control')
            if path.exists(fid):
                TestProfiles[t]['control file'] = fid
                debug("    using: %s" % fid)
                continue

            # No control file was found, this is an error.
            #
            error('Unable to find any control file for the autotest test (%s).' % t)
            raise ErrorExit()

    # determine_required_packages
    #
    def determine_required_packages(self):
        for t in self.tests_list:
            debug("Determining correct setup file for test: %s" % t)

            fid = path.join(self.autotest_tests_root, t, 'required-pkgs')
            if path.exists(fid):
                print('FOUND! setup file: %s' % fid)
                TestProfiles[t]['setup file'] = fid
                debug("    using: %s" % fid)

                try:
                    result, output = sh(fid, quiet=True)
                    TestProfiles[t]['packages'] = {}
                    TestProfiles[t]['packages']['common'] = output[0].strip().split()
                    print('>> %s <<' % TestProfiles[t]['packages'])
                except ShellError:
                    print('Failed to execute the \'required-pkgs\' file (%s)' % (fid))

    # per_test_setup
    #
    def per_test_setup(self, test):
        tp = path.join(self.saved_results_root, test)
        if path.exists(tp):
            rmtree(tp)

        self.per_test_logs_root = path.join(tp, 'logs')
        if not path.exists(self.per_test_logs_root):
            makedirs(self.per_test_logs_root)

        self.per_test_results_root = path.join(tp, 'results')

    # gather_pretest_information
    #
    def gather_pretest_information(self, test):
        pass

    # gather_posttest_information
    #
    def gather_posttest_information(self, test):
        pass

    # gather_test_results
    #
    def gather_test_results(self, test):
        user = getenv('USER')
        result, output = sh('sudo chown -R %s.%s %s' % (user, user, self.autotest_results_root))
        result, output = sh(r'sudo find %s -type d | xargs chmod +x' % (self.autotest_results_root))

        copytree(self.autotest_results_root, self.per_test_results_root)

    # proxy_check
    #
    def proxy_check(self):
        '''
        Try to determine if we need to use a proxy in order to get to the 'internet'. Set
        the appropriate environment variables if we do.
        '''
        try:
            result, output = sh('echo "" | nc -w 2 squid.internal 3128 > /dev/null 2>&1')
            environ['http_proxy']  = 'http://squid.internal:3128'
            environ['https_proxy'] = 'https://squid.internal:3128'
        except ShellError:
            try:
                result, output = sh('echo "" | nc -w 2 10.245.64.1 3128 > /dev/null 2>&1')
                environ['http_proxy']  = 'http://10.245.64.1:3128'
                environ['https_proxy'] = 'https://10.245.64.1:3128'
            except ShellError:
                pass

    # autotest
    #
    def autotest(self, test):
        '''
        '''
        self.proxy_check()

        print('update  --  update  --  update  --  update')
        sh('sudo apt-get update', ignore_result=True)
        print('update  --  update  --  update  --  update')

        # Build up the shell command that we are going to run.
        #
        cmd = 'AUTOTEST_PATH=%s sudo -E %s' % (path.join(path.expanduser('~'), 'autotest'), self.autotest_exe_path)
        if 'atargs' in TestProfiles[test]:
            cmd += ' --args="'
            for key, value in TestProfiles[test]['atargs'].iteritems():
                cmd += '%s=%s ' % (key, value)
            cmd = cmd.strip() # Remove that last space character
            cmd += '"'

        cmd += ' --verbose %s' % TestProfiles[test]['control file']

        # Run the command
        #
        result, output = sh(cmd)

        # Run the test results munger so that they are in a format that
        # we like.
        #
        result, output = sh('sudo %s %s > %s' % (path.join(self.autotest_client_root, 'tools', 'results2junit.py'), self.autotest_results_root, path.join(self.saved_results_root, 'autotest-results.%s.xml' % test)))

        return result

    # no_tests
    #
    def no_tests(self):
        print('')
        print('******************************************************************************************')
        print('** Notice:')
        print('**')
        print('**     No tests were run.')
        print('******************************************************************************************')
        print('')
        with open(path.join(self.saved_results_root, 'kernel-results.xml'), 'w') as f:
            f.write('<testsuites>\n')
            f.write('    <testsuite tests="1" errors="0" name="Runner" timestamp="2012-01-01" hostname="hostname" time="0" failures="0">\n')
            f.write('        <properties/>\n')
            f.write('        <testcase classname="Runner.No-Tests" name="runner" time="0.0">\n')
            f.write('        </testcase>\n')
            f.write('    </testsuite>\n')
            f.write('</testsuites>\n')
        with open(path.join(self.saved_results_root, 'no-tests'), 'w') as f:
            f.write('No tests were run\n')

    # gather_test_attributes
    #
    def gather_test_attributes(self):
        attributes = TestAttributes(self.args)
        attributes_path = path.join(self.saved_results_root, 'test-attributes.json')
        with open(attributes_path, 'w') as f:
            f.write(json.dumps(attributes.gather(), sort_keys=True, indent=4))
        sh('sudo cat %s' % attributes_path)

    # results_cleanup
    #
    def results_cleanup(self):
        if self.args.cloud:
            sh('sed -i -e \'s/"proc": "%s"/"proc": "%s (%s)"/\' kernel-test-results/test-attributes.json' % (self.arch, self.arch, self.args.cloud), quiet=False)
        else:
            node = Configuration['systems'][platform.node()]
            if 'proc-decoration' in node:
                sh('sed -i -e \'s/"proc": "%s"/"proc": "%s (%s)"/\' kernel-test-results/test-attributes.json' % (self.arch, self.arch, node['proc-decoration']), quiet=False)

        # Get rid of all the crash directories. These suck up a bunch of space and if anyone
        # is digging into a problem they can rerun the test by hand.
        #
        sh('find . -name \'crash.*\' -type d | xargs rm -rf')

        # Remove any/all reboot_current log directories. They add up...
        #
        sh('find . -name reboot_current -type d | xargs rm -rf')

        # Remove any/all copies of the kernel trees which some test results have in them.
        #
        sh('find . -name linux -type d | xargs rm -rf')

        return

    # main
    #
    def main(self):
        debug('Enter: main')
        retval = 1
        try:
            self.release_name
            if not self.args.cloud:
                self.tests_list_expand()
                self.tests_list_verify()

                if self.scratch_needed:
                    node = platform.node()
                    if 'scratch drive' in Configuration['systems'][node]:
                        device = Configuration['systems'][node]['scratch']
                        TestProfiles['xfstests']['atargs']['UBUNTU_SCRATCH_DEVICE'] = device

                # Some tests may require environment variables bet set. Do that.
                #
                for t in self.tests_list:
                    if 'env' in TestProfiles[t]:
                        for k, v in TestProfiles[t]['env']:
                            environ[k] = v

            else:
                self.tests_list = self.args.tests

            # Create the test-results directory where everything we care about will end
            # up.
            if not path.exists(self.saved_results_root):
                makedirs(self.saved_results_root)

            self.gather_test_attributes()

            self.test_list_determine_control_files()
            self.determine_required_packages()

            # Run the tests
            #
            t = None
            for t in self.tests_list:
                self.per_test_setup(t)

                # 1. Gather pre-test log files
                self.gather_pretest_information(t)

                # 2. Run the test
                self.autotest(t)

                # 3. Copy all the results to the results directory so they are not overwritten
                #    by subsequent test runts.
                #
                self.gather_test_results(t)

                # 4. Gather post-test log files
                self.gather_posttest_information(t)

            if t is None:
                self.no_tests()

            self.results_cleanup()

            retval = 0

        # Handle the user presses <ctrl-C>.
        #
        except KeyboardInterrupt:
            pass

        except ShellTimeoutError as e:
            error('The command (%s) timed out. (%d)' % (e.cmd, e.timeout))
            raise ErrorExit()

        except ShellError as e:
            error('The command (%s) returned a non-zero exit status (%d).' % (e.cmd, e.returncode))
            for l in e.output:
                error(l.strip())
            raise ErrorExit

        debug('Leave: main')
        return(retval)

if __name__ == '__main__':
    if getenv('DEBUG'):
        LOGLEVEL = DEBUG
    else:
        LOGLEVEL = WARNING
    LOGLEVEL = DEBUG
    basicConfig(level=LOGLEVEL, format="%(levelname)s - %(message)s")

    app_description = '''
    '''

    app_epilog = '''
    '''

    parser = ArgumentParser(description=app_description, epilog=app_epilog, formatter_class=RawDescriptionHelpFormatter)
    parser.add_argument('--dry-run',  action='store_true', default=False, help='Don\'t do anything, just print out what would be done.')
    parser.add_argument('--cloud',  help='')
    parser.add_argument('tests', metavar='T', type=str, nargs='+', help='The tests that are to be run.')
    args = parser.parse_args()

    status = 1
    try:
        app = Runner(args)
        status = app.main()
    except ErrorExit:
        error("")
        error("Due to the above error(s), this script is unable to continue and is terminating.")
        error("")
    exit(status)

# vi:set ts=4 sw=4 expandtab syntax=python:
