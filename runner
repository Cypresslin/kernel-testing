#!/usr/bin/env python
#

from os                                 import getenv, path, makedirs, environ
from sys                                import exit
from shutil                             import rmtree, copytree
from logging                            import debug, error, basicConfig, DEBUG, WARNING
from lib.argparse                       import ArgumentParser, RawDescriptionHelpFormatter
import json

from lib.testsprops                     import TestProfiles, TestCollections
from lib.test_attributes                import TestAttributes
from lib.shell                          import sh, ShellError, ShellTimeoutError

# ErrorExit
#
class ErrorExit(Exception):
    """
    If an error message has already been displayed and we want to just exit the app, this
    exception is raised.
    """
    pass

# Runner
#
class Runner():
    """
    """

    # __init__
    #
    def __init__(self, args):
        self.args = args
        self.__release_name = None
        self.__scratch_needed = None
        self.test_list = None
        self.autotest_client_root = 'autotest/client'
        self.autotest_exe_path = path.join(self.autotest_client_root, 'autotest-local')
        self.autotest_tests_root = path.join(self.autotest_client_root, 'tests')
        self.saved_results_root = 'kernel-test-results'
        self.autotest_results_root = path.join(self.autotest_client_root, 'results', 'default')
        self.per_test_logs_root = ''
        self.per_test_results_root = ''

    # tests_list_expand
    #
    def tests_list_expand(self):
        # The list of tests or the group(s) of tests that we are to
        # run is passed in on the command line.
        #
        debug('Expanding test list')
        self.tests_list = self.args.tests
        fully_expanded = False
        while not fully_expanded:
            fully_expanded = True
            neo = []
            for t in self.tests_list:
                if t in TestCollections:
                    for c in TestCollections[t]:
                        if c not in neo:
                            neo.append(c)
                        fully_expanded = False
                else:
                    if t not in neo:
                        neo.append(t)
            self.tests_list = neo
        debug('    test list: %s' % self.tests_list)

    # tests_list_verify
    #
    def tests_list_verify(self):
        errors = False

        # Verify that all the tests in the list are ones that
        # we know about.
        #
        for t in self.tests_list:
            if t not in TestProfiles:
                error('Test \'%s\' is not recognized.' % t)
                errors = True


        # Verify that certain tests have not been blaclisted for the series
        # that we are currently running.
        #
        valid_tests = []
        for t in self.tests_list:
            if 'series-blacklist' in TestProfiles[t]:
                if self.release_name in TestProfiles[t]['series-blacklist']:
                    error('Test \'%s\' has been blacklisted for this series' % t)
                else:
                    valid_tests.append(t)
            else:
                valid_tests.append(t)
        self.tests_list = valid_tests

        # Verify that the tests are valid, autotest tests.
        #
        if not path.exists(self.autotest_tests_root):
            error('The autotest client tests were not found (%s).' % (self.autotest_tests_root))
        else:
            for t in self.tests_list:
                if not path.exists(path.join(self.autotest_tests_root, t)):
                    error('The autotest test (%s) is not available in the autotest tree (%s).' % (t, self.autotest_tests_root))
                    errors = True

        if errors:
            self.tests_list = None
            raise ErrorExit()

    # release_name
    #
    @property
    def release_name(self):
        if self.__release_name is None:
            status, std_out = sh("lsb_release -a")
            for line in std_out:
                parts = line.split(":")
                if parts[0] == "Codename":
                    self.__release_name = parts[1].strip()
                    debug("Release name is %s" % self.__release_name)
                    break
        return self.__release_name

    # scratch_needed
    #
    @property
    def scratch_needed(self):
        if self.__scratch_needed is None:
            self.__scratch_needed = False
            for t in self.tests_list:
                if 'scratch' in TestProfiles[t]:
                    self.__scratch_needed = True
        debug('scratch needed: %s' % self.__scratch_needed)
        return self.__scratch_needed

    # mounts
    #
    def mounts(self):
        status, retval = sh('mount')
        return retval

    # root_device_prefix
    #
    def root_device_prefix(self):
        retval='bogus'
        for line in self.mounts():
            if ' / ' in line:
                retval = line.split()[0].replace('/dev/', '')[0:2]
                debug('The root drive prefix is: %s' % retval)
        return retval

    # test_list_determine_control_files
    #
    def test_list_determine_control_files(self):
        for t in self.tests_list:
            debug("Determining correct control file for test: %s" % t)

            # Is there control file named 'control.ubuntu.<series>'?
            #
            fid = path.join(self.autotest_tests_root, t, 'control.ubuntu.%s' % self.release_name)
            if path.exists(fid):
                TestProfiles[t]['control file'] = fid
                debug("    using: %s" % fid)
                continue

            # Is there a control file named 'control.ubuntu'?
            #
            fid = path.join(self.autotest_tests_root, t, 'control.ubuntu')
            if path.exists(fid):
                TestProfiles[t]['control file'] = fid
                debug("    using: %s" % fid)
                continue

            # Is there a control file named 'control'?
            #
            fid = path.join(self.autotest_tests_root, t, 'control')
            if path.exists(fid):
                TestProfiles[t]['control file'] = fid
                debug("    using: %s" % fid)
                continue

            # No control file was found, this is an error.
            #
            error('Unable to find any control file for the autotest test (%s).' % t)
            raise ErrorExit()

    # per_test_setup
    #
    def per_test_setup(self, test):
        tp = path.join(self.saved_results_root, test)
        if path.exists(tp):
            rmtree(tp)

        self.per_test_logs_root = path.join(tp, 'logs')
        if not path.exists(self.per_test_logs_root):
            makedirs(self.per_test_logs_root)

        self.per_test_results_root = path.join(tp, 'results')
        #if not path.exists(self.per_test_results_root):
        #    makedirs(self.per_test_results_root)

    # gather_pretest_information
    #
    def gather_pretest_information(self, test):
        pass

    # gather_posttest_information
    #
    def gather_posttest_information(self, test):
        pass

    # gather_test_results
    #
    def gather_test_results(self, test):
        user = getenv('USER')
        result, output = sh('sudo chown -R %s.%s %s' % (user, user, self.autotest_results_root))
        result, output = sh(r'sudo find %s -type d | xargs chmod +x' % (self.autotest_results_root))

        copytree(self.autotest_results_root, self.per_test_results_root)

    # install_required_packages
    #
    def install_required_packages(self):
        deps = []
        for t in self.tests_list:
            if 'packages' in TestProfiles[t]:
                if 'common' in TestProfiles[t]['packages']:
                    for pkg in TestProfiles[t]['packages']['common']:
                        deps.append(pkg)
                if self.release_name in TestProfiles[t]['packages']:
                    for pkg in TestProfiles[t]['packages'][self.release_name]:
                        deps.append(pkg)
            else:
                debug('No package dependencies for \'%s\'' % t)

        sh('sudo apt-get -fy install -fy')
        for dep in deps:
            debug('Installing: \'%s\'' % dep)
            sh('sudo apt-get install -fy %s' % dep)

    # autotest
    #
    def autotest(self, test):
        '''
        '''

        # Build up the shell command that we are going to run.
        #
        cmd = 'AUTOTEST_PATH=%s sudo -E %s' % (path.join(path.expanduser('~'), 'autotest'), self.autotest_exe_path)
        if 'atargs' in TestProfiles[test]:
            cmd += ' --args="'
            for key, value in TestProfiles[test]['atargs'].iteritems():
                cmd += '%s=%s ' % (key, value)
            cmd = cmd.strip() # Remove that last space character
            cmd += '"'

        cmd += ' --verbose %s' % TestProfiles[test]['control file']

        # Run the command
        #
        result, output = sh(cmd)

        # Run the test results munger so that they are in a format that
        # we like.
        #
        result, output = sh('sudo %s %s > %s' % (path.join(self.autotest_client_root, 'tools', 'results2junit.py'), self.autotest_results_root, path.join(self.saved_results_root, 'autotest-results.%s.xml' % test)))

        return result

    # gather_test_attributes
    #
    def gather_test_attributes(self):
        attributes = TestAttributes(self.args)
        attributes_path = path.join(self.saved_results_root, 'test-attributes.json')
        with open(attributes_path, 'w') as f:
            f.write(json.dumps(attributes.gather(), sort_keys=True, indent=4))
        sh('sudo cat %s' % attributes_path)

    # main
    #
    def main(self):
        debug('Enter: main')
        retval = 1
        try:
            self.release_name
            self.tests_list_expand()
            self.tests_list_verify()

            if self.scratch_needed:
                # We require a spare drive for xfstesting. We make an assumption about what that
                # spare drive is. Look at the device the root device is and assume that the scratch
                # drive is the 'b' device with that same device root.
                #
                TestProfiles['xfstests']['atargs']['UBUNTU_SCRATCH_DEVICE'] = '/dev/%sb' % self.root_device_prefix()

            # Some tests may require environment variables bet set. Do that.
            #
            for t in self.tests_list:
                if 'env' in TestProfiles[t]:
                    for k, v in TestProfiles[t]['env']:
                        environ[k] = v

            self.test_list_determine_control_files()

            self.install_required_packages()

            # Create the test-results directory where everything we care about will end
            # up.
            if not path.exists(self.saved_results_root):
                makedirs(self.saved_results_root)

            self.gather_test_attributes()

            # Run the tests
            #
            for t in self.tests_list:
                self.per_test_setup(t)

                # 1. Gather pre-test log files
                self.gather_pretest_information(t)

                # 2. Run the test
                self.autotest(t)

                # 3. Copy all the results to the results directory so they are not overwritten
                #    by subsequent test runts.
                #
                self.gather_test_results(t)

                # 4. Gather post-test log files
                self.gather_posttest_information(t)

            retval = 0

        # Handle the user presses <ctrl-C>.
        #
        except KeyboardInterrupt:
            pass

        #except KeyError:
        #    import json
        #    print(json.dumps(testsprops.profiles, sort_keys=True, indent=4))

        except ShellTimeoutError as e:
            error('The command (%s) timed out. (%d)' % (e.cmd, e.timeout))
            raise ErrorExit()

        except ShellError as e:
            error('The command (%s) returned a non-zero exit status (%d).' % (e.cmd, e.returncode))
            for l in e.output:
                error(l.strip())
            raise ErrorExit

        debug('Leave: main')
        return(retval)

if __name__ == '__main__':
    if getenv('DEBUG'):
        LOGLEVEL = DEBUG
    else:
        LOGLEVEL = WARNING
    LOGLEVEL = DEBUG
    basicConfig(level=LOGLEVEL, format="%(levelname)s - %(message)s")

    app_description = '''
    '''

    app_epilog = '''
    '''

    parser = ArgumentParser(description=app_description, epilog=app_epilog, formatter_class=RawDescriptionHelpFormatter)
    parser.add_argument('--dry-run',  action='store_true', default=False, help='Don\'t do anything, just print out what would be done.')
    parser.add_argument('tests', metavar='T', type=str, nargs='+', help='The tests that are to be run.')
    args = parser.parse_args()

    status = 1
    try:
        app = Runner(args)
        status = app.main()
    except ErrorExit:
        error("")
        error("Due to the above error(s), this script is unable to continue and is terminating.")
        error("")
    exit(status)

# vi:set ts=4 sw=4 expandtab syntax=python:

