#!/usr/bin/env python
#

from os                                 import getenv, path, makedirs, environ
from sys                                import exit
from shutil                             import rmtree, copytree
from logging                            import debug, error, basicConfig, DEBUG, WARNING
from lib.argparse                       import ArgumentParser, RawDescriptionHelpFormatter
import json
import platform

from lib.testsprops                     import TestProfiles, TestCollections
from lib.test_attributes                import TestAttributes
from lib.shell                          import sh, ShellError, ShellTimeoutError
from lib.configuration                  import Configuration

# ErrorExit
#
class ErrorExit(Exception):
    """
    If an error message has already been displayed and we want to just exit the app, this
    exception is raised.
    """
    pass

# Runner
#
class Runner():
    """
    """

    # __init__
    #
    def __init__(self, args):
        self.args = args
        self.__release_name = None
        self.scratch_needed = False
        self.test_list = None
        self.autotest_client_root = 'autotest/client'
        self.autotest_exe_path = path.join(self.autotest_client_root, 'autotest-local')
        self.autotest_tests_root = path.join(self.autotest_client_root, 'tests')
        self.saved_results_root = 'kernel-test-results'
        self.autotest_results_root = path.join(self.autotest_client_root, 'results', 'default')
        self.per_test_logs_root = ''
        self.per_test_results_root = ''
        self.arch = platform.processor()

    # tests_list_expand
    #
    def tests_list_expand(self):
        # The list of tests or the group(s) of tests that we are to
        # run is passed in on the command line.
        #
        self.tests_list = self.args.tests
        fully_expanded = False
        while not fully_expanded:
            fully_expanded = True
            neo = []
            for t in self.tests_list:
                if t in TestCollections:
                    for c in TestCollections[t]:
                        if c not in neo:
                            neo.append(c)
                        fully_expanded = False
                else:
                    if t not in neo:
                        neo.append(t)
            self.tests_list = neo

        # If a test requires a scratch drive make sure that this system has one. If
        # not, drop the test.
        #
        valid_tests = []
        for t in self.tests_list:
            if 'scratch' in TestProfiles[t]:
                # We require a spare drive for xfstesting. We make an assumption about what that
                # spare drive is. Look at the device the root device is and assume that the scratch
                # drive is the 'b' device with that same device root.
                #
                node = platform.node()
                if 'scratch drive' in Configuration['systems'][node] and Configuration['systems'][node]['scratch drive']:
                    valid_tests.append(t)
                    self.__scratch_needed = True
            else:
                valid_tests.append(t)
        self.tests_list = valid_tests

        if len(self.tests_list) > 0:
            debug('Expanded test list:')
            for t in self.tests_list:
                debug('    %s' % t)

    def blacklisted_by_series(self, profile):
        retval = False
        if 'series-blacklist' in profile:
            if self.release_name in profile['series-blacklist']:
                retval = True
        return retval

    def blacklisted_by_arch(self, profile):
        retval = False
        if 'arch-blacklist' in profile:
            if self.arch in profile['arch-blacklist']:
                retval = True
        return retval

    # tests_list_verify
    #
    def tests_list_verify(self):
        debug('Enter tests_list_verify')

        errors = False

        # Verify that all the tests in the list are ones that
        # we know about.
        #
        for t in self.tests_list:
            if t not in TestProfiles:
                error('Test \'%s\' is not recognized.' % t)
                errors = True

        # Verify that certain tests have not been blaclisted for the series
        # that we are currently running.
        #
        valid_tests = []
        debug('    blacklist checking (%s) (%s)' % (self.release_name, self.arch))
        for t in self.tests_list:
            debug('        \'%s\'' % t)
            if self.blacklisted_by_series(TestProfiles[t]):
                error('Test \'%s\' has been blacklisted for this series (%s)' % (t, self.release_name))
            elif self.blacklisted_by_arch(TestProfiles[t]):
                error('Test \'%s\' has been blacklisted for this arch (%s)' % (t, self.arch))
            else:
                debug('            valid')
                valid_tests.append(t)

        self.tests_list = valid_tests


        # Verify that the tests are valid, autotest tests.
        #
        if not path.exists(self.autotest_tests_root):
            error('The autotest client tests were not found (%s).' % (self.autotest_tests_root))
        else:
            for t in self.tests_list:
                if not path.exists(path.join(self.autotest_tests_root, t)):
                    error('The autotest test (%s) is not available in the autotest tree (%s).' % (t, self.autotest_tests_root))
                    errors = True

        if errors:
            self.tests_list = None
            raise ErrorExit()

        if len(self.tests_list) > 0:
            debug('Verified test list:')
            for t in self.tests_list:
                debug('    %s' % t)

        debug('Leave tests_list_verify')

    # release_name
    #
    @property
    def release_name(self):
        if self.__release_name is None:
            status, std_out = sh("lsb_release -a")
            for line in std_out:
                parts = line.split(":")
                if parts[0] == "Codename":
                    self.__release_name = parts[1].strip()
                    debug("Release name is %s" % self.__release_name)
                    break
        return self.__release_name

    # test_list_determine_control_files
    #
    def test_list_determine_control_files(self):
        for t in self.tests_list:
            debug("Determining correct control file for test: %s" % t)

            # Is there control file named 'control.ubuntu.<series>'?
            #
            fid = path.join(self.autotest_tests_root, t, 'control.ubuntu.%s' % self.release_name)
            if path.exists(fid):
                TestProfiles[t]['control file'] = fid
                debug("    using: %s" % fid)
                continue

            # Is there a control file named 'control.ubuntu'?
            #
            fid = path.join(self.autotest_tests_root, t, 'control.ubuntu')
            if path.exists(fid):
                TestProfiles[t]['control file'] = fid
                debug("    using: %s" % fid)
                continue

            # Is there a control file named 'control'?
            #
            fid = path.join(self.autotest_tests_root, t, 'control')
            if path.exists(fid):
                TestProfiles[t]['control file'] = fid
                debug("    using: %s" % fid)
                continue

            # No control file was found, this is an error.
            #
            error('Unable to find any control file for the autotest test (%s).' % t)
            raise ErrorExit()

    # determine_required_packages
    #
    def determine_required_packages(self):
        for t in self.tests_list:
            debug("Determining correct setup file for test: %s" % t)

            fid = path.join(self.autotest_tests_root, t, 'required-pkgs')
            if path.exists(fid):
                print('FOUND! setup file: %s' % fid)
                TestProfiles[t]['setup file'] = fid
                debug("    using: %s" % fid)

                try:
                    result, output = sh(fid, quiet=True)
                    TestProfiles[t]['packages'] = {}
                    TestProfiles[t]['packages']['common'] = output[0].strip().split()
                    print('>> %s <<' % TestProfiles[t]['packages'])
                except ShellError:
                    print('Failed to execute the \'required-pkgs\' file (%s)' % (fid))

    # per_test_setup
    #
    def per_test_setup(self, test):
        tp = path.join(self.saved_results_root, test)
        if path.exists(tp):
            rmtree(tp)

        self.per_test_logs_root = path.join(tp, 'logs')
        if not path.exists(self.per_test_logs_root):
            makedirs(self.per_test_logs_root)

        self.per_test_results_root = path.join(tp, 'results')
        #if not path.exists(self.per_test_results_root):
        #    makedirs(self.per_test_results_root)

    # gather_pretest_information
    #
    def gather_pretest_information(self, test):
        pass

    # gather_posttest_information
    #
    def gather_posttest_information(self, test):
        pass

    # gather_test_results
    #
    def gather_test_results(self, test):
        user = getenv('USER')
        result, output = sh('sudo chown -R %s.%s %s' % (user, user, self.autotest_results_root))
        result, output = sh(r'sudo find %s -type d | xargs chmod +x' % (self.autotest_results_root))

        copytree(self.autotest_results_root, self.per_test_results_root)

    # proxy_check
    #
    def proxy_check(self):
        '''
        Try to determine if we need to use a proxy in order to get to the 'internet'. Set
        the appropriate environment variables if we do.
        '''
        try:
            result, output = sh('echo "" | nc -w 2 squid.internal 3128 > /dev/null 2>&1')
            environ['http_proxy']  = 'http://squid.internal:3128'
            environ['https_proxy'] = 'https://squid.internal:3128'
        except ShellError as e:
            try:
                result, output = sh('echo "" | nc -w 2 10.245.64.1 3128 > /dev/null 2>&1')
                environ['http_proxy']  = 'http://10.245.64.1:3128'
                environ['https_proxy'] = 'https://10.245.64.1:3128'
            except ShellError as e:
                pass

    # autotest
    #
    def autotest(self, test):
        '''
        '''
        self.proxy_check()

        # Build up the shell command that we are going to run.
        #
        cmd = 'AUTOTEST_PATH=%s sudo -E %s' % (path.join(path.expanduser('~'), 'autotest'), self.autotest_exe_path)
        if 'atargs' in TestProfiles[test]:
            cmd += ' --args="'
            for key, value in TestProfiles[test]['atargs'].iteritems():
                cmd += '%s=%s ' % (key, value)
            cmd = cmd.strip() # Remove that last space character
            cmd += '"'

        cmd += ' --verbose %s' % TestProfiles[test]['control file']

        # Run the command
        #
        result, output = sh(cmd)

        # Run the test results munger so that they are in a format that
        # we like.
        #
        result, output = sh('sudo %s %s > %s' % (path.join(self.autotest_client_root, 'tools', 'results2junit.py'), self.autotest_results_root, path.join(self.saved_results_root, 'autotest-results.%s.xml' % test)))

        return result

    # gather_test_attributes
    #
    def gather_test_attributes(self):
        attributes = TestAttributes(self.args)
        attributes_path = path.join(self.saved_results_root, 'test-attributes.json')
        with open(attributes_path, 'w') as f:
            f.write(json.dumps(attributes.gather(), sort_keys=True, indent=4))
        sh('sudo cat %s' % attributes_path)

    # main
    #
    def main(self):
        debug('Enter: main')
        retval = 1
        try:
            self.release_name
            self.tests_list_expand()
            self.tests_list_verify()

            if self.scratch_needed:
                node = platform.node()
                if 'scratch drive' in Configuration['systems'][node]:
                    device = Configuration['systems'][node]['scratch']
                    TestProfiles['xfstests']['atargs']['UBUNTU_SCRATCH_DEVICE'] = device

            # Some tests may require environment variables bet set. Do that.
            #
            for t in self.tests_list:
                if 'env' in TestProfiles[t]:
                    for k, v in TestProfiles[t]['env']:
                        environ[k] = v

            # Create the test-results directory where everything we care about will end
            # up.
            if not path.exists(self.saved_results_root):
                makedirs(self.saved_results_root)

            self.gather_test_attributes()

            self.test_list_determine_control_files()
            self.determine_required_packages()

            # Run the tests
            #
            for t in self.tests_list:
                self.per_test_setup(t)

                # 1. Gather pre-test log files
                self.gather_pretest_information(t)

                # 2. Run the test
                self.autotest(t)

                # 3. Copy all the results to the results directory so they are not overwritten
                #    by subsequent test runts.
                #
                self.gather_test_results(t)

                # 4. Gather post-test log files
                self.gather_posttest_information(t)

            retval = 0

        # Handle the user presses <ctrl-C>.
        #
        except KeyboardInterrupt:
            pass

        #except KeyError:
        #    import json
        #    print(json.dumps(testsprops.profiles, sort_keys=True, indent=4))

        except ShellTimeoutError as e:
            error('The command (%s) timed out. (%d)' % (e.cmd, e.timeout))
            raise ErrorExit()

        except ShellError as e:
            error('The command (%s) returned a non-zero exit status (%d).' % (e.cmd, e.returncode))
            for l in e.output:
                error(l.strip())
            raise ErrorExit

        debug('Leave: main')
        return(retval)

if __name__ == '__main__':
    if getenv('DEBUG'):
        LOGLEVEL = DEBUG
    else:
        LOGLEVEL = WARNING
    LOGLEVEL = DEBUG
    basicConfig(level=LOGLEVEL, format="%(levelname)s - %(message)s")

    app_description = '''
    '''

    app_epilog = '''
    '''

    parser = ArgumentParser(description=app_description, epilog=app_epilog, formatter_class=RawDescriptionHelpFormatter)
    parser.add_argument('--dry-run',  action='store_true', default=False, help='Don\'t do anything, just print out what would be done.')
    parser.add_argument('tests', metavar='T', type=str, nargs='+', help='The tests that are to be run.')
    args = parser.parse_args()

    status = 1
    try:
        app = Runner(args)
        status = app.main()
    except ErrorExit:
        error("")
        error("Due to the above error(s), this script is unable to continue and is terminating.")
        error("")
    exit(status)

# vi:set ts=4 sw=4 expandtab syntax=python:

