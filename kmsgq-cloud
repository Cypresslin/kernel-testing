#!/usr/bin/env python3
#
# kmsgq-jenkins
#   An app that listens for RabbitMQ messages related to kernel testing requests
#   and processes them.
#
#   This app is multithreaded. The messages are taken from the message queue and
#   put a queue for the thread to then processes. This is done because it may be
#   necessary to send messages to the message queue as part of the processing and
#   that can't be done in the same context as the msgq handler.
#

import sys
sys.path.insert(0, 'lib3')
from os                                 import path
from time                               import sleep
from argparse                           import ArgumentParser, RawDescriptionHelpFormatter
from logging                            import basicConfig, DEBUG, INFO, WARNING
import signal
import traceback
import json
import threading
import queue
from lib.log                            import center, cleave, Clog, cinfo, cdebug
from lib3.msgq                          import MsgQueue
from lib3.cloud                         import CloudJobFactory, AWSJobFactory

# Exit
#
class Exit(Exception):
    """
    If an error message has already been displayed and we want to just exit the app, this
    exception is raised.
    """
    pass

class RequestHandler(threading.Thread):

    # __init__
    #
    def __init__(s, cmd_q=queue.Queue(), reply_q=queue.Queue(), cfg={}, msgq=None):
        threading.Thread.__init__(s)
        s.cmd_q = cmd_q
        s.reply_q = reply_q
        s.msgq = msgq
        s.alive = threading.Event()
        s.alive.set()
        s.cfg = cfg

    # store_jobs
    #
    def store_jobs(s, request, jobs):
        center(s.__class__.__name__ + '.store_jobs')

        for j in jobs:
            del jobs[j]['description']  # Don't need this
            del jobs[j]['op']
            del jobs[j]['pocket']
            del jobs[j]['ssh_options']
            del jobs[j]['arch']
            del jobs[j]['who']
        msg = {}
        msg['key']  = 'kernel.testing.jobs.created'
        msg['series-name'] = request['series-name']
        msg['kernel-version'] = request['kernel-version']
        msg['jobs'] = jobs
        msg['timestamp'] = request['date']
        msg['package'] = request['package']
        msg['series-name'] = request['series-name']
        msg['hwe'] = request['hwe']
        file_name  = ''
        file_name += request['series-name']
        file_name += '__'
        file_name += request['kernel-version']
        file_name += '__'
        file_name += request['op']
        file_name += '.msg'
        file_name  = path.join(path.expanduser('~'), 'job-messages', file_name)
        with open(file_name, "w") as fid:
            fid.write(json.dumps(msg, sort_keys=True, indent=4))
            fid.write('\n')
        cleave(s.__class__.__name__ + '.store_jobs')
        return msg

    # handle_request
    #
    def handle(s, request):
        center(s.__class__.__name__ + '.handle')
        cinfo('        ---------------------------------------  h a n d l e  ---------------------------------------', 'green')
        cinfo(json.dumps(request, sort_keys=True, indent=4))

        # For now all clouds are amd64. sut-arch is required
        #
        request['arch'] = 'amd64'

        if 'cloud' in request:
            cloud = request['cloud']
        else:
            cloud = request['flavour']
            if cloud in ('gke', 'gcp'):
                cloud = 'gce'

        if cloud == 'aws':
            factory = AWSJobFactory(cloud=cloud, series=request['series-name'], region=s.cfg[cloud]['region'], tests=cloud, request=request)
        else:
            factory = CloudJobFactory(cloud=cloud, series=request['series-name'], region=s.cfg[cloud]['region'], tests=cloud, request=request)

        jobs = factory.create_jobs()
        msg = s.store_jobs(request, jobs)

        # Announce the list of jobs before we start them.
        #
        if s.cfg['args'].local_msgqueue_port:
            mq = MsgQueue(address='localhost', port=9123)
        else:
            mq = MsgQueue()
        mq.publish(msg['key'], msg)

        factory.start_jobs(jobs)

        cleave(s.__class__.__name__ + '.handle')

    # dequeue
    #
    def dequeue(s):
        # bjf center(s.__class__.__name__ + '.dequeue')
        try:
            # Queue.get with timeout to allow checking s.alive
            request = s.cmd_q.get(True, 0.1)
            s.handle(request)

        except queue.Empty:
            pass
        # bjf cleave(s.__class__.__name__ + '.dequeue')

    # run
    #
    def run(s):
        center(s.__class__.__name__ + '.run')
        while s.alive.isSet():
            s.dequeue()
            sleep(10)
        cleave(s.__class__.__name__ + '.run')


# JobFactory
#
class JobFactory(object):
    '''
    '''

    # __init__
    #
    def __init__(s):
        '''
        '''
        center(s.__class__.__name__ + '.__init__')
        cleave(s.__class__.__name__ + '.__init__')

    # sigterm_handler
    #
    def sigterm_handler(s, signu, frame):
        center(s.__class__.__name__ + '.sigterm_handler')
        raise Exit()
        cleave(s.__class__.__name__ + '.sigterm_handler')

    # sighup_handler
    #
    def sighup_handler(s, signu, frame):
        center(s.__class__.__name__ + '.sighup_handler')
        cleave(s.__class__.__name__ + '.sighup_handler')

    # enqueue
    #
    def enqueue(s, payload):
        center(s.__class__.__name__ + '.enqueue')
        s.rh.cmd_q.put(payload)
        cleave(s.__class__.__name__ + '.enqueue')

    # msgq_handler
    #
    def msgq_handler(s, payload):
        center(s.__class__.__name__ + '.msgq_handler')

        valid_keys = ['kernel.testing.request', 'kernel.publish']
        #valid_ops  = ['aws', 'gce', 'azure', 'gke']
        #valid_pkgs = ['linux', 'linux-aws', 'linux-azure', 'linux-gke']
        valid_ops  = ['sru']
        valid_pkgs = ['linux-azure', 'linux-gke', 'linux-aws', 'linux-gcp']

        # Determine if the key in the current message is on that
        # we are going to process.
        #
        for key in valid_keys:
            if not payload['key'].startswith(key):
                continue

            if payload['op'] not in valid_ops:
                continue

            if payload['package'] not in valid_pkgs:
                continue

            s.enqueue(payload)
            break

        cleave(s.__class__.__name__ + '.msgq_handler')

    # main
    #
    def main(s, args):
        center(s.__class__.__name__ + '.main')
        retval = 0
        s.args = args

        signal.signal(signal.SIGTERM, s.sigterm_handler)
        signal.signal(signal.SIGHUP,  s.sighup_handler)

        try:
            if args.local_msgqueue_port:
                # s.mq = MsgQueue(address='localhost', port=args.local_msgqueue_port)
                mq = MsgQueue(address='localhost', port=9123)
            else:
                mq = MsgQueue()

            cfg = {
                'aws' : {
                    # 'region' : 'us-west-1',
                    'region' : 'us-west-2',
                },
                'gce' : {
                    'region' : 'us-west1-a',
                },
                'azure' : {
                    'region' : 'West US',
                },
            }

            cfg['args'] = args

            s.rh = RequestHandler(msgq=mq, cfg=cfg)
            s.rh.start()

            # queue = 'kernel-aws-publishing-monitor'
            # mq.queue_delete(queue)
            # queue = 'kernel-gce-publishing-monitor'
            # mq.queue_delete(queue)
            # queue = 'kernel-azure-publishing-monitor'
            # mq.queue_delete(queue)

            queue = 'kernel-cloud-publishing-monitor'
            mq.listen(queue, 'kernel.#', s.msgq_handler)

            retval = 0

        # Handle the user presses <ctrl-C>.
        #
        except KeyboardInterrupt:
            print("Aborting ...")
            s.rh.alive.clear()

        except Exit:
            print("")
            print("Due to the above error(s), this script is unable to continue and is terminating.")
            print("")
            s.rh.alive.clear()

        cleave(s.__class__.__name__ + '.main (%s)' % retval)
        return retval

if __name__ == '__main__':
    retval = -1

    # Command line argument setup and initial processing
    #
    app_description = '''
    '''
    app_epilog = '''
    '''
    parser = ArgumentParser(description=app_description, epilog=app_epilog, formatter_class=RawDescriptionHelpFormatter)
    parser.add_argument('--debug', action='store_true', default=False, help='')
    parser.add_argument('--info',  action='store_true', default=False, help='')
    parser.add_argument('--lkp',   action='store_true', default=False, help='Handle requests for live kernel patching testing.')
    parser.add_argument('--dev',   action='store_true', default=False, help='Handle development testing requests.')
    parser.add_argument('--local-msgqueue-port',  type=int, default=None,  help='The local port to be used to talk to the Rabbit MQ service.')

    args = parser.parse_args()

    # If logging parameters were set on the command line, handle them
    # here.
    #
    Clog.color = True
    if args.debug:
        log_format = "%(levelname)s - %(message)s"
        basicConfig(level=DEBUG, format=log_format)
        Clog.dbg = True
    elif args.info:
        log_format = "%(message)s"
        basicConfig(level=INFO, format=log_format)
    else:
        log_format = "%(message)s"
        basicConfig(level=WARNING, format=log_format)

    center('__main__')
    try:
        app = JobFactory()
        retval = app.main(args)
    except KeyboardInterrupt:
        pass
    except Exception:
        trace = traceback.format_exc()
        try:
            sys.stderr.write(trace)
        except:
            pass
        logfile = open(path.join('.', 'exceptions.log'), 'a')
        logfile.write('Critical exception in %s' % sys.argv[0])
        logfile.write(trace)
        logfile.write('----------------------------------------\n\n')
        logfile.close()

    cleave('__main__ (%s)' % (retval))
    exit(retval)

# vi:set ts=4 sw=4 expandtab syntax=python:
